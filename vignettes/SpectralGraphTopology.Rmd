---
title: "Learning graphs from data via spectral constraints"
author: |
  | Ze Vinicius, Daniel P. Palomar, Jiaxi Ying, and Sandeep Kumar
  | Hong Kong University of Science and Technology (HKUST)
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
header-includes:
  \usepackage[]{algorithm2e}
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Learning the topology of graphs}
  %\VignetteKeyword{graph, topology, clustering}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96,
  pngquant = "--speed=1"
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/SpectralGraphTopology.Rmd", "all")
#rmarkdown::render("vignettes/SpectralGraphTopology.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/SpectralGraphTopology.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/SpectralGraphTopology.pdf", gs_quality = "ebook")
```

-----------

# Package Snapshot

**spectralGraphTopology** contains a collection of numerous implementations of
state-of-the-art algorithms designed to estimate graph matrices
(Laplacian and Adjacency) from data, including:

* Structured Graph Laplacian (SGL): S. Kumar, J. Ying, J. V. de Miranda Cardoso, and D. P. Palomar (2019). A unified framework for structured graph learning via spectral constraints. https://arxiv.org/abs/1904.09792
* Combinatorial Graph Laplacian (CGL): H. E. Egilmez, E. Pavez and A. Ortega, "Graph Learning From Data Under Laplacian and Structural Constraints", in IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 825-841, Sept. 2017.
* Constrained Laplacian Rank (CLR): N., Feiping, W., Xiaoqian, J., Michael I., and H., Heng. (2016). The Constrained Laplacian Rank Algorithm for Graph-based Clustering, AAAI’16.
* Graph Laplacian Estimation (GLE-MM and GLE-ADMM): Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P. Palomar, Optimization Algorithms for Graph Laplacian Estimation via ADMM and MM, IEEE Trans. on Signal Processing, vol. 67, no. 16, pp. 4231-4244, Aug. 2019


# Installation

**spectralGraphTopology** can 

Check out [https://mirca.github.io/spectralGraphTopology](https://mirca.github.io/spectralGraphTopology)
for installation instructions.


# Vanilla case: no structural constraints
Graphs are arguably one of the most popular mathematical structures
that find applications in a myriad of scientific and engineering fields,
such as finance, medical imaging, transportation, networks, and so on.

In the era of big data, graphs can be used to model a vast diversity of
phenomena, including customer preferences, brain activity, genetic structures, the stock market,
just to name a few. Therefore, it is of utmost importance to be able to
reliably estimate graph structures from noisy, often sparse, low-rank datasets.

The Laplacian matrix of a graph contains the information
about its topology, i.e., how nodes are connected among themselves. By definition
a (combinatorial) Laplacian matrix is positive semi-definite, symmetric, and with sum of rows
equal to zero.

One common approach to estimate the Laplacian matrix of a graph (without satisfying the zero row-sum property)
would be via the generalized inverse of the sample covariance matrix, which is an assymptotically unbiased
and efficient estimator. In this document, we call this approach the naive one. In R, this estimator can be
computed simply as ``MASS::ginv(cov(Y))``, where ``Y`` is the ``n x p`` data matrix, where ``n`` is the
number of samples (or features) and ``p`` is the number of nodes. However, this estimator
performs very poorly when the sample size ``n`` is small when compared with the number of nodes ``p``, which
makes its use questionable for practical purposes.

Another classical approach, the well-known graphical lasso algorithm, was proposed in
[@Friedman2008] where a maximum likelihood (ML) estimation was formulated under a Gaussian Markov random field (GMRF) model including an $\ell_1$-norm penalty in order to induce
sparsity on the solution.  More precisely, the mathematical formulation for the graphical lasso problem
can be expressed as follows:
$$\begin{array}{ll}
\underset{\boldsymbol{\Theta} \succ \mathbf{0}}{\textsf{maximize}} & \log \det \boldsymbol{\Theta}
- \mathrm{tr}(\mathbf{S}\boldsymbol{\Theta}) + \alpha \Vert \boldsymbol{\Theta}\Vert_{1},
\end{array}
$$
where $\mathbf{S}$ is the sample covariance matrix (or feature inner product matrix) and $\alpha$ is a hyperparameter
that controls the amount of sparsity of the solution. The solution is a sparse precision matrix (still not satisfying the zero row-sum property). This estimator has been efficiently implemented in the R package ``glasso``.

In [@Licheng2019], the authors extended the graphical lasso framework so as to
consider $\boldsymbol{\Theta}$ as a graph Laplacian. Mathematically, the set of (combinatorial) Laplacian
matrices may be written as
$$
\begin{equation}
\mathcal{S}_{\mathcal{L}} = \left\{\boldsymbol{\Theta} \in \mathbb{R}^{p \times p}: \boldsymbol{\Theta}\mathbf{1} = \mathbf{0},
\boldsymbol{\Theta}_{ij} = \boldsymbol{\Theta}_{ji} \le 0, \boldsymbol{\Theta} \succeq \mathbf{0}\right\}
\end{equation}
$$

In addition, the authors in [@Licheng2019] assumed that the binary adjacency matrix is known
a priori. Such assumption can be justified in many practical cases. For example,
in social networks, the undirected graph of a public user A is determined by the set of users followed by A
and who also follow A.
Another example arises in the field of finance, where stocks in the same industry or sector
are naturally more likely to behave in a similar fashion.  For those cases, we can assume that the
binary adjacency matrix of the graph is known a priori and we can use state-of-the-art algorithms
that leverages this fact to estimate the weighted relationship between nodes (edges). In this package,
we implement the methods named GLE-MM and GLE-ADMM that were proposed in [@Licheng2019].


[Nice comparison plot of convergence: Ortega's method, Licheng's method, our method simplified to just one subproblem.]





# Introducing structural constraints
Although graphical lasso and its extensions have shown an extreme success across many scientific fields,
they usually only allow for the estimation of connected graphs.  In many practical situations, however,
graphs more complex structures need to be estimated, such as k-component graphs, which is
are widely used in clustering tasks.
Additionally, the structure of the graph might be known a priori, i.e., whether the graph
is k-component, bipartite, k-component bipartite, etc.

In this sense, in [@Kumar2019], we included spectral constraints into the
regularized maximum likelihood framework as follows:
$$\begin{array}{ll}
\underset{\boldsymbol{\Theta}}{\textsf{maximize}} & \log \mathrm{gdet} \boldsymbol{\Theta}
- \mathrm{tr}(\mathbf{S}\boldsymbol{\Theta}) + \alpha \Vert \boldsymbol{\Theta}\Vert_{1}, \\
\textsf{subject to} & \boldsymbol{\Theta} \in \mathcal{S}_{\mathcal{L}},
\end{array}
$$
where $\textrm{gdet}$ denotes the generalized determinant (the product of positive eigenvalues)
and $\mathcal{S}_{\mathcal{L}}$ represents the set of positive semi-definite graph
Laplacian matrices, i.e.,

Now, realizing that for any $\boldsymbol{\Theta} \in \mathcal{S}_{\mathcal{L}}$, $\boldsymbol{\Theta} = \mathcal{L}\mathbf{w}$,
where $\mathcal{L} : \mathbb{R}^{p(p-1)/2} \rightarrow \boldsymbol{\Theta}$ is a linear operator
that maps a non-negative vector of edge weights $\mathbf{w}$ into a Laplacian matrix $\boldsymbol{\Theta}$,
and that $\boldsymbol{\Theta}$ can be factorized as $\boldsymbol{\Theta} = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^\top$,
the original optimization problem may be approximated as follows:
$$\begin{array}{ll}
\underset{\mathbf{w}, \boldsymbol{\lambda}, \mathbf{U}}{\textsf{minimize}} & - \log \textrm{gdet}\left({\sf Diag}(\boldsymbol{\lambda})\right)
+ \mathrm{tr}\left(\mathbf{S}\mathcal{L}\mathbf{w}\right) + \alpha \Vert\mathcal{L}\mathbf{w}\Vert_{1}
+ \frac{\beta}{2}\big\|\mathcal{L}\mathbf{w} - \mathbf{U}{\sf Diag}(\boldsymbol{\lambda})\mathbf{U}^{T}\big\|^{2}_{F}\\
\textsf{subject to} & \mathbf{w} \geq 0, \boldsymbol{\lambda} \in \mathcal{S}_{\boldsymbol{\Lambda}},~\text{and}~
\mathbf{U}^{T}\mathbf{U} = \mathbf{I}
\end{array}$$
where $\mathcal{S}_{\boldsymbol{\Lambda}}$ is the set of vectors that constrains the eigenvalues of
the Laplacian matrix. For example, for a $k$-component graph with $p$ nodes,
$\mathcal{S}_{\boldsymbol{\Lambda}} = \left\{\{\lambda_i\}_{i=1}^{p} | \lambda_1 = \lambda_2 = \cdots = \lambda_k = 0,\;
0 < \lambda_{k+1} \leq \lambda_{k+2} \leq \cdots \leq \lambda_{p} \right\}$.

To solve this optimization problem, we employ a block majorization-minimization framework that updates each
of the variables ($\mathbf{w}, \boldsymbol{\lambda}, \mathbf{U}$) at once while fixing the remaning ones.
For the mathematical details of the solution, including a convergence proof, please refer to our paper [@Kumar2019].

In order to learn bipartite graphs, we take advantage of the fact that the eigenvalues of the adjacency matrix
of graph are symmetric around 0, and we formulate the following optimization problem:
$$
\begin{array}{ll}
\underset{{\mathbf{w}},{\boldsymbol{\psi}},{\mathbf{V}}}{\textsf{minimize}} &
\begin{array}{c}
- \log \det (\mathcal{L} \mathbf{w}+\frac{1}{p}\mathbf{11}^{T})+\text{tr}({\mathbf{S}\mathcal{L} \mathbf{w}})+
  \alpha \Vert\mathcal{L}\mathbf{w}\Vert_{1}+
  \frac{\gamma}{2}\Vert \mathcal{A} \mathbf{w}-\mathbf{V} {\sf Diag}(\boldsymbol{\psi}) \mathbf{V}^T \Vert_F^2,
\end{array}\\
\text{subject to} & \begin{array}[t]{l}
\mathbf{w} \geq 0, \ \boldsymbol{\psi} \in \mathcal{S}_{\boldsymbol{\psi}}, \ \text{and} \ \mathbf{V}^T\mathbf{V}=\mathbf{I},
\end{array}
\end{array}
$$

In a similar fashion, we construct the optimization problem to estimate a $k$-component bipartite graph
by combining the constraints related to the Laplacian and adjacency matrices.






# Package usage

The `spectralGraphTopology` package provides three main functions to estimate k-component, bipartite, and
k-component bipartite graphs, respectively: `learn_k_component_graph`, `learn_bipartite_graph`, and
`learn_bipartite_k_component_graph`.  In the next subsections, we will check out how to apply those functions
in synthetic datasets.

## Learning a grid graph
```{r, message=FALSE}
library(spectralGraphTopology)
library(igraph)
library(viridis)
set.seed(0)

# generate the graph and the data from the graph
p <- 64
grid <- make_lattice(length = sqrt(p), dim = 2)
n <- as.integer(100 * p)
E(grid)$weight <- runif(gsize(grid), min = 1e-1, max = 3)
L_true <- as.matrix(laplacian_matrix(grid))  # true Laplacian matrix
Y <- MASS::mvrnorm(n, mu = rep(0, p), Sigma = MASS::ginv(L_true))

# estimate the graph
S <- cov(Y)
graph <- learn_k_component_graph(S, w0 = "qp", beta = 20, alpha = 5e-3,
                                 abstol = 1e-5, verbose = FALSE)
graph$Adjacency[graph$Adjacency < 5e-2] <- 0
estimated_grid <- graph_from_adjacency_matrix(graph$Adjacency,
                                              mode = "undirected", weighted = TRUE)

# plots
colors <- viridis(20, begin = 0, end = 1, direction = -1)
c_scale <- colorRamp(colors)
E(estimated_grid)$color = apply(
  c_scale(E(estimated_grid)$weight / max(E(estimated_grid)$weight)), 1,
                          function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
E(grid)$color = apply(c_scale(E(grid)$weight / max(E(grid)$weight)), 1,
                      function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
V(estimated_grid)$color = "grey"
V(grid)$color = "grey"
la <- layout_on_grid(grid)
par(mfrow = c(1, 2))
plot(grid, layout = la, vertex.label = NA, vertex.size = 3)
title("True grid graph")
plot(estimated_grid, layout = la, vertex.label = NA, vertex.size = 3)
title("Estimated grid graph")
```

## Learning a 3-component graph

The next snippet of code shows how to learn the clusters of a set of points
distributed on the plane.

```{r, message=FALSE}
library(spectralGraphTopology)
library(clusterSim)
library(igraph)
set.seed(42)

# generate the graph and the data from the graph
n <- 100  # number of nodes per cluster
circles3 <- shapes.circles3(n)  # generate datapoints
k <- 3  # number of components
S <- crossprod(t(circles3$data))  # compute sample correlation matrix

# estimate the graph
graph <- learn_k_component_graph(S, k = k, beta = 1, verbose = FALSE,
                                 fix_beta = FALSE, abstol = 1e-3)

# plots
# build network
net <- graph_from_adjacency_matrix(graph$Adjacency, mode = "undirected", weighted = TRUE)
# colorify nodes and edges
colors <- c("#706FD3", "#FF5252", "#33D9B2")
V(net)$cluster <- circles3$clusters
E(net)$color <- apply(as.data.frame(get.edgelist(net)), 1,
                      function(x) ifelse(V(net)$cluster[x[1]] == V(net)$cluster[x[2]],
                                        colors[V(net)$cluster[x[1]]], '#000000'))
V(net)$color <- colors[circles3$clusters]
plot(net, layout = circles3$data, vertex.label = NA, vertex.size = 3)
title("Estimated graph on the three circles dataset")
```

Similar structures may be inferred as well. The plots below depict the results of applying
`learn_k_component_graph()` on a variaety of spatially distributed points.

```{r, echo=FALSE, out.width = "25%", fig.show = "hold", fig.align = "default"}
knitr::include_graphics(c("figures/circles2.png", "figures/helix3d.png",
                          "figures/twomoon.png", "figures/worms.png"))
```

## Learning a bipartite graph
```{r, message=FALSE}
library(spectralGraphTopology)
library(igraph)
library(viridis)
library(corrplot)
set.seed(42)

# generate the graph and the data from the graph
n1 <- 10
n2 <- 6
n <- n1 + n2
pc <- .9
bipartite <- sample_bipartite(n1, n2, type="Gnp", p = pc, directed=FALSE)
# randomly assign edge weights to connected nodes
E(bipartite)$weight <- runif(gsize(bipartite), min = 0, max = 1)
# get true Laplacian and Adjacency
Ltrue <- as.matrix(laplacian_matrix(bipartite))
Atrue <- diag(diag(Ltrue)) - Ltrue
# get samples
Y <- MASS::mvrnorm(100 * n, rep(0, n), Sigma = MASS::ginv(Ltrue))

# estimate graph
S <- cov(Y)  # compute sample covariance matrix
graph <- learn_bipartite_graph(S, z = 4, verbose = FALSE)
graph$Adjacency[graph$Adjacency < 1e-3] <- 0

# plots
par(mfrow = c(1, 2))
corrplot(Atrue / max(Atrue), is.corr = FALSE, method = "square", addgrid.col = NA,
         tl.pos = "n", cl.cex = 1, mar=c(0, 0, 3, 2))
title("True Adjacency")
corrplot(graph$Adjacency / max(graph$Adjacency), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1, mar=c(0, 0, 3, 2))
title("Estimated Adjacency")
# build networks
estimated_bipartite <- graph_from_adjacency_matrix(graph$Adjacency, mode = "undirected",
                                                   weighted = TRUE)
V(estimated_bipartite)$type <- c(rep(0, 10), rep(1, 6))
la = layout_as_bipartite(estimated_bipartite)
colors <- viridis(20, begin = 0, end = 1, direction = -1)
c_scale <- colorRamp(colors)
E(estimated_bipartite)$color = apply(
   c_scale(E(estimated_bipartite)$weight / max(E(estimated_bipartite)$weight)), 1,
                                     function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
E(bipartite)$color = apply(c_scale(E(bipartite)$weight / max(E(bipartite)$weight)), 1,
                      function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
la = la[, c(2, 1)]
# Plot networks: true and estimated
par(mfrow = c(1, 2))
plot(bipartite, layout = la, vertex.color=c("red","black")[V(bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
title("True bipartite graph")
plot(estimated_bipartite, layout = la,
     vertex.color=c("red","black")[V(estimated_bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(estimated_bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
title("Estimated Bipartite Graph")
```

## Learning a 2-component bipartite graph
```{r, message=FALSE}
library(spectralGraphTopology)
library(igraph)
library(viridis)
library(corrplot)
set.seed(42)

# generate the graph and the data from the graph
w <- c(1, 0, 0, 1, 0, 1) * runif(6)
Laplacian <- block_diag(L(w), L(w))
Atrue <- diag(diag(Laplacian)) - Laplacian
bipartite <- graph_from_adjacency_matrix(Atrue, mode = "undirected", weighted = TRUE)
n <- ncol(Laplacian)

# estimate graph
Y <- MASS::mvrnorm(40 * n, rep(0, n), MASS::ginv(Laplacian))
graph <- learn_bipartite_k_component_graph(cov(Y), k = 2, beta = 1e2, nu = 1e2,
                                           verbose = FALSE)
graph$Adjacency[graph$Adjacency < 1e-2] <- 0

# plots
# Plot Adjacency matrices: true and estimated
par(mfrow = c(1, 2))
corrplot(Atrue / max(Atrue), is.corr = FALSE, method = "square", addgrid.col = NA,
         tl.pos = "n", cl.cex = 1, mar=c(0, 0, 3, 2))
title("True Adjacency")
corrplot(graph$Adjacency / max(graph$Adjacency), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1, mar=c(0, 0, 3, 2))
title("Estimated Adjacency")
# Plot networks
estimated_bipartite <- graph_from_adjacency_matrix(graph$Adjacency, mode = "undirected",
                                                   weighted = TRUE)
V(bipartite)$type <- rep(c(TRUE, FALSE), 4)
V(estimated_bipartite)$type <- rep(c(TRUE, FALSE), 4)
la = layout_as_bipartite(estimated_bipartite)
colors <- viridis(20, begin = 0, end = 1, direction = -1)
c_scale <- colorRamp(colors)
E(estimated_bipartite)$color = apply(
   c_scale(E(estimated_bipartite)$weight / max(E(estimated_bipartite)$weight)), 1,
                                     function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
E(bipartite)$color = apply(c_scale(E(bipartite)$weight / max(E(bipartite)$weight)), 1,
                           function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
la = la[, c(2, 1)]
# Plot networks: true and estimated
par(mfrow = c(1, 2))
plot(bipartite, layout = la,
     vertex.color = c("red","black")[V(bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
title("True block bipartite graph")
plot(estimated_bipartite, layout = la,
     vertex.color = c("red","black")[V(estimated_bipartite)$type + 1],
     vertex.shape = c("square", "circle")[V(estimated_bipartite)$type + 1],
     vertex.label = NA, vertex.size = 5)
title("Estimated block bipartite graph")
```

# Performance comparison

We use the following baseline algorithms for performance comparison:

1. the generalized inverse of the sample covariance matrix, denoted as Naive;
2. a quadratic program estimator given by $\textsf{min}_\mathbf{w}\;\Vert\mathbf{S}^{\dagger} - \mathcal{L}\mathbf{w}\Vert_{F}$, where $\mathbf{S}^{\dagger}$ is the generalized inverse of the sample covariance matrix, denoted as QP;
3. the Combinatorial Graph Laplacian proposed by [@Egilmez2017] denoted as CGL.

The plots below show the performance in terms of F-score and relative error among the proposed algorithm, denoted as SGL,
and the baseline ones when learning a grid graph with 64 nodes and edges uniformly drawn from the interval [.1, 3].  For each algorithm,
the shaded area and the solid curve represent the standard deviation and the mean of several Monte Carlo realizations.
It can be noticed that SGL outperforms all the baseline algorithms in all sample size regimes.  Such superior
performance maybe be attributed to the highly structured nature of grid graphs.

```{r, echo=FALSE, out.width = "60%"}
knitr::include_graphics("figures/fscore_grid.png")
knitr::include_graphics("figures/relative_error_grid.png")
```

In a similar fashion, the plots below shows algorithmic performance for modular graphs with 64 nodes and 4 modules,
such that the probability of connection within module was set to 50%, whereas the probability of connection accross
modules was set to 1%. In this scenario, SGL outperforms the baselines algorithms QP and Naive, while having a similar
performance to that of CGL.  This may be explained by the fact that the edges connecting nodes do not quite have a
deterministic structure like those of the grid graphs.

```{r, echo=FALSE, out.width = "60%"}
knitr::include_graphics("figures/fscore_modular.png")
knitr::include_graphics("figures/relative_error_modular.png")
```

# Clustering

One of the most direct applications of learning k-component graphs is on the
classical unsupervised machine learning problem: data clustering. For this task, we make use of two datasets:
the animals dataset [@osherson1991] the Cancer RNA-Seq dataset [@Dua2017].

The animals dataset consists of binary answers to questions such as “is warm-blooded?,” “has lungs?”, etc.
There are a total of 102 such questions, which make up the features for 33 animal categories.

The cancer-RNA Seq dataset consists of genetic features which map 5 types of cancer: breast carcinoma (BRCA),
kidney renal clear-cell carcinoma (KIRC), lung adenocarcinoma (LUAD), colon adenocarcinoma (COAD), and prostate
adenocarcinoma (PRAD). This dataset consists of 801 labeled samples, in which every sample has 20531 genetic features.

The clustering results for these datasets are shown below. The code used for the cancer-rna dataset
can be found at our GitHub repo: https://github.com/dppalomar/spectralGraphTopology/tree/master/benchmarks/cancer-rna

```{r, message=FALSE}
library(pals)

# load data
df <- read.csv("animals-dataset/features.txt", header = FALSE)
names <- matrix(unlist(read.csv("animals-dataset/names.txt", header = FALSE)))
Y <- t(matrix(as.numeric(unlist(df)), nrow = nrow(df)))
p <- ncol(Y)

# estimate graph
graph <- learn_k_component_graph(cov(Y) + diag(1/3, p, p), w0 = "qp", beta = 1, k = 10,
                                 verbose = FALSE)

# plots
net <- graph_from_adjacency_matrix(graph$Adjacency, mode = "undirected", weighted = TRUE)
colors <- brewer.reds(100)
c_scale <- colorRamp(colors)
E(net)$color = apply(c_scale(abs(E(net)$weight) / max(abs(E(net)$weight))), 1,
                     function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
V(net)$color = "pink"
plot(net, vertex.label = names,
     vertex.size = 3,
     vertex.label.dist = 1,
     vertex.label.family = "Helvetica",
     vertex.label.cex = .8,
     vertex.label.color = "black")
```

```{r, echo=FALSE, out.width = "40%"}
knitr::include_graphics("figures/cancer-rna-graph.png")
```

# Appendix I: Experiments with GLE-MM and GLE-ADMM


As a code example, graph estimation is carried out as follows:

```{r}
library(igraph)
library(spectralGraphTopology)
library(pals)
set.seed(42)

# number of nodes
p <- 100
# number of modules
m <- 4
# Prefix matrix
Prefix <- diag(.2, 4) + 0.0025
# Generate a modular graph
mgraph <- sample_sbm(p, pref.matrix = Prefix, block.sizes = c(rep(p / m, m)))
# Randomly assign weights to the edges
E(mgraph)$weight <- runif(gsize(mgraph), min = 1e-1, max = 3)
# Get the true Laplacian and Adjacency matrices
Ltrue <- as.matrix(laplacian_matrix(mgraph))
Atrue <- diag(diag(Ltrue)) - Ltrue
# Get the unweighted true Adjacency matrix (assumed to be known)
A <- 1 * (Ltrue < 0)
# Generate samples from the Laplacian matrix
Y <- MASS::mvrnorm(10 * p, mu = rep(0, p), Sigma = MASS::ginv(Ltrue))
# Compute the sample covariance matrix
S <- cov(Y)
# Estimate a graph from the samples using the MM method
graph <- learn_laplacian_gle_mm(S = S, A = A,
                                record_objective = TRUE, verbose = FALSE)
# Check convergence status
graph$convergence
# Compute the relative error between true and estimated Laplacians
relative_error(Ltrue, graph$Laplacian)
# Plot objective function per iteration
plot(c(1:length(graph$obj_fun)), graph$obj_fun, xlab = "Iteration number",
     ylab = "Objective function value", type = "b", pch=15, lty=1, cex=.75, col = "#6ABA81")
# Compute estimated adjacency matrix
Aest <- diag(diag(graph$Laplacian)) - graph$Laplacian
# Plot estimated graph
net <- graph_from_adjacency_matrix(Aest, mode = "undirected", weighted = TRUE)
colors <- brewer.reds(100)
c_scale <- colorRamp(colors)
E(net)$color = apply(c_scale(abs(E(net)$weight) / max(abs(E(net)$weight))), 1,
                     function(x) rgb(x[1]/255, x[2]/255, x[3]/255))
V(net)$color = "pink"
plot(net, vertex.size = 3, vertex.label = NA)
# Plot true and estimated Adjacency matrices
par(mfrow = c(1, 2))
corrplot(Atrue / max(Atrue), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1.25)
title("True Adjacency")
corrplot(Aest / max(Aest), is.corr = FALSE, method = "square",
         addgrid.col = NA, tl.pos = "n", cl.cex = 1.25)
title("Estimated Adjacency")
```

As discussed in [@Licheng2019], the estimation performance of GLE-MM and GLE-ADMM are
practically the same. The toy example below shows the relative error between the
estimation given by the MM and ADMM methods and the true Laplacian as the number of
samples increase.
```{r, cache = TRUE}
library(spectralGraphTopology)
library(igraph)
set.seed(42)

# sample ratios
ratios <- c(5, 10, 100, 250, 500, 1000)
# number of nodes
p <- 100
# number of modules
m <- 4
# Prefix matrix
Prefix <- diag(.2, 4) + 0.025
# relative errors between the true Laplacian and the estimated ones
re_mm <- rep(0, length(ratios))
re_admm <- rep(0, length(ratios))
for (k in c(1:length(ratios))) {
  # Generate a modular graph
  mgraph <- sample_sbm(p, pref.matrix = Prefix, block.sizes = c(rep(p / m, m)))
  # Randomly assign weights to the edges
  E(mgraph)$weight <- runif(gsize(mgraph), min = 1e-1, max = 3)
  # Get the true Laplacian and Adjacency matrices
  Ltrue <- as.matrix(laplacian_matrix(mgraph))
  Atrue <- diag(diag(Ltrue)) - Ltrue
  # Get the unweighted true Adjacency matrix (assumed to be known)
  A <- 1 * (Ltrue < 0)
  # Generate samples from the Laplacian matrix
  Y <- MASS::mvrnorm(ratios[k] * p, mu = rep(0, p), Sigma = MASS::ginv(Ltrue))
  # Compute the sample covariance matrix
  S <- cov(Y)
  # Estimate a graph from the samples using the MM method
  graph_mm <- learn_laplacian_gle_mm(S = S, A = A, verbose = FALSE)
  # Estimate a graph from the samples using the ADMM method
  graph_admm <- learn_laplacian_gle_admm(S = S, A = A, verbose = FALSE)
  # record relative error between true and estimated Laplacians
  re_mm[k] <- relative_error(Ltrue, graph_mm$Laplacian)
  re_admm[k] <- relative_error(Ltrue, graph_admm$Laplacian)
}
xlab <- latex2exp::TeX("$\\mathit{n} / \\mathit{p}$")
colors <- c("#0B032D", "#843B62")
pch <- c(11, 7)
legend <- c("MM", "ADMM")
plot(c(1:length(ratios)), re_mm, ylim=c(0, max(re_mm) + 0.01), xlab = xlab,
     ylab = "Relative Error", type = "b", pch=pch[1], lty=1,
     cex=.75, col = colors[1], xaxt = "n")
lines(c(1:length(ratios)), re_admm, type = "b", lty=1, pch=pch[2],
      cex=.75, col = colors[2], xaxt = "n")
axis(side = 1, at = c(1:length(ratios)), labels = ratios)
legend("topright", legend=legend, col=colors, pch=pch, lty=c(1, 1), bty="n")
```

The plot above confirms that the performance of both algorithms are in fact substantially similar.

Now, let's try to reproduce part of the Figure 3 in [@Licheng2019]:
```{r}
library(spectralGraphTopology)
library(igraph)

# number of nodes
p <- seq(5, 50, 5)
re_naive <- rep(0, length(p))
re_gle_mm <- rep(0, length(p))
re_gle_admm <- rep(0, length(p))

for (k in c(1:length(p))) {
  w <- runif(.5 * p[k] * (p[k] - 1))
  Ltrue <- L(w)
  A <- 1 * (Ltrue < 0)
  # Generate samples from the Laplacian matrix
  Y <- MASS::mvrnorm(100 * p[k], mu = rep(0, p[k]), Sigma = MASS::ginv(Ltrue))
  # Compute the sample covariance matrix
  S <- cov(Y)
  Lnaive <- MASS::ginv(S)
  # Estimate a graph from the samples using the MM method
  graph_mm <- learn_laplacian_gle_mm(S = S, A = A, verbose = FALSE)
  # Estimate a graph from the samples using the ADMM method
  graph_admm <- learn_laplacian_gle_admm(S = S, A = A, verbose = FALSE)
  # record relative error between true and estimated Laplacians
  re_naive[k] <- relative_error(Ltrue, Lnaive)
  re_mm[k] <- relative_error(Ltrue, graph_mm$Laplacian)
  re_admm[k] <- relative_error(Ltrue, graph_admm$Laplacian)
}
xlab <- latex2exp::TeX("$\\mathit{p}$")
colors <- c("#0B032D", "#843B62", "#6c5ce7")
pch <- c(11, 7, 8)
legend <- c("MM", "ADMM", "Naive")
plot(c(1:length(p)), re_mm, ylim=c(min(re_naive, re_mm, re_admm), max(re_naive, re_mm, re_admm)), xlab = xlab,
     ylab = "Relative Error", type = "b", pch=pch[1], lty=1,
     cex=.75, col = colors[1], xaxt = "n")
lines(c(1:length(p)), re_admm, type = "b", lty=1, pch=pch[2],
      cex=.75, col = colors[2], xaxt = "n")
lines(c(1:length(p)), re_naive, type = "b", lty=1, pch=pch[3],
      cex=.75, col = colors[3], xaxt = "n")
axis(side = 1, at = c(1:length(p)), labels = p)
legend("topright", legend=legend, col=colors, pch=pch, lty=c(1, 1, 1), bty="n")
```

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
