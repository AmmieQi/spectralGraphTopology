---
title: "Learning the topology of graphs"
author: "Convex Group-HKUST"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
header-includes:
  \usepackage[]{algorithm2e}
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Learning the topology of graphs}
  %\VignetteKeyword{graph, topology, clustering}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "all")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/SpectralGraphTopology-vignette.pdf", gs_quality = "ebook")
```

-----------

# Installation

For installation instructions, please visit
[https://github.com/dppalomar/spectralGraphTopology](https://github.com/dppalomar/spectralGraphTopology)

# Problem Statement
The Laplacian matrix $\boldsymbol{\Theta}$ of a graph contains the information of its topology and weight connections. By definition a Laplacian matrix is positive semi-definite, symmetric, and with sum of rows equal to zero. The Laplacian linear operator $\mathcal{L}\mathbf{w}$ maps a vector of weights $\mathbf{w}$ into a valid Laplacian matrix so that the conditions are satisfied by construction.

The underlying optimization problem of learning a graph may be expressed as follows:
$$\begin{array}{ll}
\underset{\mathbf{w}, \boldsymbol{\Lambda}, \mathbf{U}}{\textsf{minimize}} & - \log \det(\boldsymbol{\Lambda})
+ \mathrm{tr}\left(\mathbf{K}\mathcal{L}\mathbf{w}\right)
+ \frac{\beta}{2}\big\|\mathcal{L}\mathbf{w} - \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{T}\big\|^{2}_{F}\\
\textsf{subject to} & \mathbf{w} \geq 0, \boldsymbol{\Lambda} \in \mathcal{S}_{\boldsymbol{\Lambda}},~\text{and}~
\mathbf{U}^{T}\mathbf{U} = \mathbf{I}
\end{array}$$
where $\mathcal{S}_{\boldsymbol{\Lambda}}$ further constrains the eigenvalues of Laplacian matrices
$\boldsymbol{\Theta}=\mathcal{L}\mathbf{w}$ according to some topology (e.g., a $K$-component graph has $K$ zero eigenvalues).

In order to solve this problem, we use a block coordinate descent algorithm to
iteratively optimize each variable while helding the others fixed.

# Usage of the package
We illustrate the usage of the package with simulated data, as follows:
```{r, cache = TRUE}
library(spectralGraphTopology)
set.seed(123)

# Number of samples
T <- 200
# Vector to generate the Laplacian matrix of the graph
w <- runif(10)
# Laplacian matrix
Theta <- L(w)
# Sample data from a Multivariate Gaussian
N <- ncol(Theta)
Y <- MASS::mvrnorm(T, rep(0, N), MASS::ginv(Theta))
# Number of components of the graph
K <- 1
# Learn the Laplacian matrix
res <- learnGraphTopology(Y, K, beta = 10)
```

Let's visually inspect the true Laplacian and the estimated one:
```{r}
Theta
res$Theta
```

We can evaluate the performance of the learning process in a more objective manner
by computing the relative error between the true Laplacian matrix and the estimated one,
which can be done as follows:
```{r}
norm(Theta - res$Theta, type="F") / norm(Theta, type="F")

Theta_naive <- MASS::ginv(cov(Y))
norm(Theta - Theta_naive, type="F") / norm(Theta, type="F")
```
In this case, the naive estimation of the Laplacian matrix (i.e., generalized inverse of the sample covariance matrix) performs already quite well since the ratio $T/N$ is large enough `r T/N` for the sample covariance matrix to be accurately estimated.

Let's also look at the convergence of the objective function versus iterations:
```{r}
N_iter <- length(res$fun)
plot(c(1:N_iter), res$fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Objective function")

```

For K > 1, we can generate the Laplacian as a block diagonal matrix, as follows
```{r}
library(spectralGraphTopology)
T <- 200
w1 <- runif(3)
w2 <- runif(3)
Theta1 <- L(w1)
Theta2 <- L(w2)
N1 <- ncol(Theta1)
N2 <- ncol(Theta2)
Theta <- rbind(cbind(Theta1, matrix(0, N1, N2)),
               cbind(matrix(0, N2, N1), Theta2))
Y <- MASS::mvrnorm(T, rep(0, N1 + N2), MASS::ginv(Theta))
K <- 2
res <- learnGraphTopology(Y, K, beta = 10)
norm(Theta - res$Theta, type="F") / norm(Theta, type="F")
norm(Theta - MASS::ginv(cov(Y)), type="F") / norm(Theta, type="F")
```

```{r}
Theta
res$Theta
```

```{r}
N_iter <- length(res$fun)
plot(c(1:N_iter), res$fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Objective function")
```
```{r}
plot(c(1:N_iter), res$loglike, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Log Likelihood")
```



# Explanation of the algorithms

In this section we describe in detail the algorithms designed to solve the
graph topology learning problem.

## `learnGraphTopology`: Learning the topology of graph

The goal of `learnGraphTopology()` is to estimate the Laplacian matrix generated
by the weight vector of a graph, $\mathbf{w}$. The algorithm for the function `learnGraphTopology`
is stated as follows:

\begin{algorithm}[H]
 \DontPrintSemicolon
 \KwData{$\mathbf{Y}$ (data matrix), $K$ (\#\{components\}), $\beta$ (regularization term),
 $\mathbf{w}^{(0)}$, $\boldsymbol{\lambda}^{(0)}$, $\mathbf{U}^{(0)}$ (initial parameter estimates),
 $\alpha_1$, $\alpha_2$ (lower and upper bound on the eigenvalues of the Laplacian matrix),
 $\rho$ (how much to increase beta per iteration)}
 \KwResult{$\boldsymbol{\Theta}$ (Laplacian matrix)}
 $N \gets \texttt{ncol}(\mathbf{Y})$\;
 \While{objective function do not converged or max \#\{iterations\} not reached}{
   $k \gets 0$\;
   \While{parameters do not converged or max \#\{iterations\} not reached}{
  $\mathbf{w}^{(k+1)} \gets \texttt{w\_update}(\mathbf{w}^{(k)}, \mathbf{U}^{(k)}, \boldsymbol{\lambda}^{(k)}, \beta, N, \mathbf{K})$\;
  $\mathbf{U}^{(k+1)} \gets \texttt{U\_update}(\mathbf{w}^{(k+1)}, N)$\;
  $\boldsymbol{\lambda}^{(k+1)} \gets \texttt{lambda\_update}(\mathbf{w}^{(k+1)},
  \mathbf{U}^{(k+1)}, \alpha_1, \alpha_2, \beta,N, K)$\;
  $k \gets k + 1$\;
  }
  $\beta \gets \beta (\rho + 1)$\;
 }
 \Return $\mathcal{L}(\mathbf{w}^{(k+1)})$\;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwupdate}{\texttt{w\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwupdate{$\mathbf{w}, \mathbf{U}, \boldsymbol{\lambda}, \beta, N, \mathbf{K}$}}{
        $\nabla_{\mathbf{w}}f \gets \mathcal{L}^{\star}\left(\mathcal{L}\left(\mathbf{w}\right)
                        - \mathbf{U}  \texttt{diag}(\boldsymbol{\lambda}) {\mathbf{U}^{T}}
                        + \dfrac{\mathbf{K}}{\beta}\right)$\;
        \KwRet $\texttt{max}\left(0, \mathbf{w} - \dfrac{\nabla_{\mathbf{w}}f}{2N}\right)$\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FwUupdate}{\texttt{U\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FwUupdate{$\mathbf{w}, N$}}{
        \KwRet \texttt{eigenvectors}$(\mathcal{L}(\mathbf{w}))$ \# \textit{increasing order w.r.t. eigenvalues}\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwlambdaupdate}{\texttt{lambda\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwlambdaupdate{$\mathbf{w}, \mathbf{U}, \alpha_1, \alpha_2, \beta, N, K$}}{
        $\mathbf{d} \gets \texttt{diag}\left(\mathbf{U}^{T} \mathcal{L}(\mathbf{w})\mathbf{U}\right)$\;
        $\boldsymbol{\lambda} \gets \frac{1}{2}\left(\mathbf{d} +
                \sqrt{\mathbf{d} \odot \mathbf{d} + \frac{4}{\beta}}\right)$
                \# $\odot$ \textit{means element-wise multiplication} \;
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order
             \textbf{and} $\min(\boldsymbol{\lambda}) \geq \alpha_1$
             \textbf{and} $\max(\boldsymbol{\lambda}) \leq \alpha_2$} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            set to $\alpha_1$ the elements of $\boldsymbol{\lambda}$
            whose values are less than $\alpha_1$ \;
            set to $\alpha_2$ the elements of $\boldsymbol{\lambda}$
            whose values are greater than $\alpha_2$\;
        }
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            $\mathbf{raise}$ \texttt{Exception}(\textit{"eigenvalues are not in increasing order"})\;
        }
  }
  \;
\end{algorithm}

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
