---
title: "Learning the topology of graphs"
author: "Convex Group-HKUST"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: leonids
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
header-includes:
  \usepackage[]{algorithm2e}
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Learning the topology of graphs}
  %\VignetteKeyword{graph, topology, clustering}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "all")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/SpectralGraphTopology-vignette.pdf", gs_quality = "ebook")
```

-----------

# Installation

For installation instructions, please visit
[https://github.com/dppalomar/spectralGraphTopology](https://github.com/dppalomar/spectralGraphTopology)

# Problem Statement
The Laplacian matrix $\boldsymbol{\Theta}$ of a graph contains the information
of its topology and weight connections. By definition a Laplacian matrix is
positive semi-definite, symmetric, and with sum of rows equal to zero. The
Laplacian linear operator $\mathcal{L}\mathbf{w}$ maps a vector of weights
$\mathbf{w}$ into a valid Laplacian matrix so that the conditions are satisfied
by construction.

One common approach to estimate the Laplacian matrix would be via the generalized
inverse of the covariance matrix, which is an assymptotically unbiased and efficient
estimator. In this document, we call this approach the \textit{naive} one.
In \textsf{R}, this estimator can be computed as ``MASS::ginv(cov(Y))``,
where``Y`` is the data matrix.

Another classical approach was proposed in [@Friedman2008] which incorporates a
$\ell_1$-norm penalty term in order to induce sparsity on the solution. The R package
``glasso`` provides an implementation of this estimator.

The underlying optimization problem of learning a K-component graph, solved by
``spectralGraphTopology``, may be expressed as follows:
$$\begin{array}{ll}
\underset{\mathbf{w}, \boldsymbol{\Lambda}, \mathbf{U}}{\textsf{minimize}} & - \log \det(\boldsymbol{\Lambda})
+ \mathrm{tr}\left(\mathbf{K}\mathcal{L}\mathbf{w}\right)
+ \frac{\beta}{2}\big\|\mathcal{L}\mathbf{w} - \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{T}\big\|^{2}_{F}\\
\textsf{subject to} & \mathbf{w} \geq 0, \boldsymbol{\Lambda} \in \mathcal{S}_{\boldsymbol{\Lambda}},~\text{and}~
\mathbf{U}^{T}\mathbf{U} = \mathbf{I}
\end{array}$$
where $\mathcal{S}_{\boldsymbol{\Lambda}}$ further constrains the eigenvalues of Laplacian matrices
$\boldsymbol{\Theta}=\mathcal{L}\mathbf{w}$ according to some topology (e.g., a $K$-component graph has $K$ zero eigenvalues).

In order to solve this problem, we use a block coordinate descent algorithm to
iteratively optimize each variable while helding the others fixed. For
details check the paper or the algorithm description at the end of this document.

# Usage of the package
We illustrate the usage of the package with simulated data, as follows:
```{r, cache = TRUE}
library(spectralGraphTopology)
set.seed(123)

# Number of samples
T <- 200
# Vector to generate the Laplacian matrix of the graph
w <- runif(10)
# Laplacian matrix
Theta <- L(w)
# Sample data from a Multivariate Gaussian
N <- ncol(Theta)
Y <- MASS::mvrnorm(T, rep(0, N), MASS::ginv(Theta))
# Number of components of the graph
K <- 1
# Learn the Laplacian matrix
res <- learnGraphTopology(Y, K, beta = 10)
```

Let's visually inspect the true Laplacian and the estimated one:
```{r}
Theta
res$Theta
```

Another visual tool is the correlation matrix:

```{r, cache = TRUE}
library(corrplot)
par(mfrow = c(1, 2))
corrplot(cov2cor(Theta))
title("True Laplacian matrix", line = 2.5)
corrplot(cov2cor(res$Theta))
title("Estimated Laplacian matrix", line = 2.5)
```

Finally, we can also plot the graph structure:
```{r}
library(qgraph)
par(mfrow = c(1, 2))
qgraph(Theta - diag(diag(Theta)), esize = 5, edge.color = "black")
title("True diamond graph", line = 2.5)
qgraph(res$Theta - diag(diag(res$Theta)), esize = 5, edge.color = "black")
title("Estimated diamond graph", line = 2.5)
```

We evaluate the performance of the learning process in a more objective
manner by computing two criterions:

1. relative error
2. percentage improvement in average loss (PRIAL)

```{r, cache = TRUE}
relativeError <- function(Xtrue, Xest) {
    return (100 * norm(Xtrue - Xest, type = "F") / norm(Xtrue, type = "F"))
}

prial <- function(Xtrue, Xest) {
    Xnaive <- MASS::ginv(cov(Xtrue))
    return (100 * (1 - (norm(Xest - Xtrue, type = "F") /
                        norm(Xnaive - Xtrue, type = "F"))^2))
}
```


```{r}
Theta_naive <- MASS::ginv(cov(Y))
rel_err <- c(proposed = relativeError(Theta, res$Theta),
             naive = relativeError(Theta, Theta_naive))
prial_avg <- c(proposed = prial(Theta, res$Theta),
               naive = prial(Theta, Theta_naive))
rel_err
prial_avg
```

In this case, the naive estimation of the Laplacian matrix (i.e., generalized inverse of the sample covariance matrix) performs already quite well since the ratio $T/N$ is large enough `r T/N` for the sample covariance matrix to be accurately estimated.

Let's also look at the convergence of the objective function versus iterations:
```{r}
N_iter <- length(res$fun)
plot(c(1:N_iter), res$fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Objective function")

```

For K > 1, we can generate the Laplacian as a block diagonal matrix, as follows
```{r, cache = TRUE}
library(spectralGraphTopology)
T <- 500
w1 <- runif(3)
w2 <- runif(6)
Theta1 <- L(w1)
Theta2 <- L(w2)
N1 <- ncol(Theta1)
N2 <- ncol(Theta2)
Theta <- blockDiag(list(Theta1, Theta2))
Y <- MASS::mvrnorm(T, rep(0, N1 + N2), MASS::ginv(Theta))
K <- 2
res <- learnGraphTopology(Y, K, beta = 10)
```
```{r}
Theta_naive <- MASS::ginv(cov(Y))
rel_err <- c(proposed = relativeError(Theta, res$Theta),
             naive = relativeError(Theta, Theta_naive))
prial_avg <- c(proposed = prial(Theta, res$Theta),
               naive = prial(Theta, Theta_naive))
rel_err
prial_avg
```

```{r}
Theta
res$Theta
```

```{r}
N_iter <- length(res$fun)
plot(c(1:N_iter), res$fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Objective function")
```
```{r}
plot(c(1:N_iter), res$loglike, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "Iteration number", ylab = "Negative Log Likelihood")
```

```{r}
library(qgraph)
par(mfrow = c(1, 2))
qgraph(Theta - diag(diag(Theta)), esize = 5, edge.color = "black")
title("True graph", line = 2.5)
qgraph(res$Theta - diag(diag(res$Theta)), esize = 5, edge.color = "black")
title("Estimated graph", line = 2.5)
```

# Explanation of the algorithms

In this section we present the algorithms designed to solve the
graph topology learning problem.

## `learnGraphTopology`: Learning the topology of graph

The goal of `learnGraphTopology` is to estimate the Laplacian matrix generated
by the weight vector of a graph, $\mathbf{w}$. The algorithm for the function `learnGraphTopology`
is stated as follows:

\begin{algorithm}[H]
 \DontPrintSemicolon
 \KwData{$\mathbf{Y}$ (data matrix), $K$ (\#\{components\}), $\beta$ (regularization term),
 $\mathbf{w}^{(0)}$, $\boldsymbol{\lambda}^{(0)}$, $\mathbf{U}^{(0)}$ (initial parameter estimates),
 $\alpha_1$, $\alpha_2$ (lower and upper bound on the eigenvalues of the Laplacian matrix),
 $\rho$ (how much to increase beta per iteration).}
 \KwResult{$\boldsymbol{\Theta}$ (Laplacian matrix)}
 $N \gets \texttt{ncol}(\mathbf{Y})$\;
 \While{objective function do not converged \textbf{or} max \#\{iterations\} not reached}{
   $k \gets 0$\;
   \While{parameters do not converged \textbf{or} max \#\{iterations\} not reached}{
  $\mathbf{w}^{(k+1)} \gets \texttt{w\_update}(\mathbf{w}^{(k)}, \mathbf{U}^{(k)}, \boldsymbol{\lambda}^{(k)}, \beta, N, \mathbf{K})$\;
  $\mathbf{U}^{(k+1)} \gets \texttt{U\_update}(\mathbf{w}^{(k+1)}, N)$\;
  $\boldsymbol{\lambda}^{(k+1)} \gets \texttt{lambda\_update}(\mathbf{w}^{(k+1)},
  \mathbf{U}^{(k+1)}, \alpha_1, \alpha_2, \beta,N, K)$\;
  $k \gets k + 1$\;
  }
  $\beta \gets \beta (\rho + 1)$\;
 }
 \Return $\mathcal{L}(\mathbf{w}^{(k+1)})$\;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwupdate}{\texttt{w\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwupdate{$\mathbf{w}, \mathbf{U}, \boldsymbol{\lambda}, \beta, N, \mathbf{K}$}}{
        $\nabla_{\mathbf{w}}f \gets \mathcal{L}^{\star}\left(\mathcal{L}\left(\mathbf{w}\right)
                        - \mathbf{U}  \texttt{diag}(\boldsymbol{\lambda}) {\mathbf{U}^{T}}
                        + \dfrac{\mathbf{K}}{\beta}\right)$\;
        \KwRet $\texttt{max}\left(0, \mathbf{w} - \dfrac{\nabla_{\mathbf{w}}f}{2N}\right)$\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FwUupdate}{\texttt{U\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FwUupdate{$\mathbf{w}, N, K$}}{
        \KwRet \texttt{eigenvectors}$(\mathcal{L}(\mathbf{w}))$[K+1:N] \# \textit{increasing order w.r.t. eigenvalues}\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwlambdaupdate}{\texttt{lambda\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwlambdaupdate{$\mathbf{w}, \mathbf{U}, \alpha_1, \alpha_2, \beta, N, K$}}{
        $\mathbf{d} \gets \texttt{diag}\left(\mathbf{U}^{T} \mathcal{L}(\mathbf{w})\mathbf{U}\right)$\;
        $\boldsymbol{\lambda} \gets \frac{1}{2}\left(\mathbf{d} +
                \sqrt{\mathbf{d} \odot \mathbf{d} + \frac{4}{\beta}}\right)$
                \# $\odot$ \textit{means element-wise multiplication} \;
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order
             \textbf{and} $\min(\boldsymbol{\lambda}) \geq \alpha_1$
             \textbf{and} $\max(\boldsymbol{\lambda}) \leq \alpha_2$} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            set to $\alpha_1$ the elements of $\boldsymbol{\lambda}$
            whose values are less than $\alpha_1$ \;
            set to $\alpha_2$ the elements of $\boldsymbol{\lambda}$
            whose values are greater than $\alpha_2$\;
        }
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            $\mathbf{raise}$ \texttt{Exception}(\textit{"eigenvalues are not in increasing order"})\;
        }
  }
  \;
\end{algorithm}

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
