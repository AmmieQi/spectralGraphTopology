---
title: "Learning the topology of graphs"
author: "Convex Group-HKUST"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
header-includes:
  \usepackage[]{algorithm2e}
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Learning the topology of graphs}
  %\VignetteKeyword{graph, topology, clustering}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "all")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/SpectralGraphTopology-vignette.pdf", gs_quality = "ebook")
```

-----------


# Usage of the package
We illustrate the usage of the package with simulated data, as follows::
```{r, cache = TRUE}
library(spectralGraphTopology)
set.seed(123)

# Number of samples
T <- 10000
# Vector to generate the Laplacian matrix of the graph
w <- runif(10)
# Laplacian matrix
Theta <- L(w)
# Sample data from a Multivariate Gaussian
N <- ncol(Theta)
Y <- MASS::mvrnorm(T, rep(0, N), MASS::ginv(Theta))
# Number of components of the graph
K <- 1
# Learn the Laplacian matrix
res <- learnGraphTopology(Y, K, beta = 10)
```

Let's visually inspect the true Laplacian and the estimated one as follows::
```{r}
Theta
res$Theta
```

We can evaluate the performance of the learning process in a more objective manner
by computing the relative error between the true Laplacian matrix and the estimated one,
which can be done as follows::
```{r}
RE <- norm(Theta - res$Theta, type="F") / max(1., norm(Theta, type="F"))
RE
```

Let's also look at the trend of the objective function per iteration:
```{r}
k <- length(res$fun)
plot(c(1:k), res$fun,  pch=19, cex=.5, col = scales::alpha("black", .25),
     xlab = "Iteration number", ylab = "Objective function")
```

For K > 1, we can generate the Laplacian as a block diagonal matrix, as follows
```{r}
library(spectralGraphTopology)
w1 <- runif(3)
w2 <- runif(3)
Theta1 <- L(w1)
Theta2 <- L(w2)
N1 <- ncol(Theta1)
N2 <- ncol(Theta2)
Theta <- rbind(cbind(Theta1, matrix(0, N1, N2)),
               cbind(matrix(0, N2, N1), Theta2))
Y <- MASS::mvrnorm(T, rep(0, N1 + N2), MASS::ginv(Theta))
K <- 2
beta <- 5
res <- learnGraphTopology(Y, K, beta = beta, ftol = 1e-3)
RE <- norm(Theta - res$Theta, type="F") / max(1., norm(Theta, type="F"))
RE
```

```{r}
Theta
res$Theta
```

As we can observe, the matrices' structure do not quite match.

I suspect they might be similar matrices. Let's check some properties:

```{r}
eigen(Theta, only.values = TRUE)
eigen(res$Theta, only.values = TRUE)
```

```{r}
sum(diag(Theta))
sum(diag(res$Theta))
```

```{r}
det(Theta)
det(res$Theta)
```

```
N <- N1 + N2
objFunction(Theta, res$U, res$lambda, res$Km, beta, N, K)
objFunction(res$Theta, res$U, res$lambda, res$Km, beta, N, K)
```


```{r}
k <- length(res$fun)
plot(c(1:k), res$fun, pch=19, cex=.6, xlab = "Iteration number",
     ylab = "Objective function")
```



# Explanation of the algorithms

In this section we describe in detail the algorithms designed to solve the
graph topology learning problem.

## `learnGraphTopology`: Learning the topology of graph

The goal of `learnGraphTopology()` is to estimate the Laplacian matrix generated
by the weight vector of a graph, $\mathbf{w}$. The algorithm for the function `learnGraphTopology`
is stated as follows:

\begin{algorithm}[H]
 \DontPrintSemicolon
 \KwData{$\mathbf{Y}$ (data matrix), $K$ (\#\{components\}), $\beta$ (regularization term),
 $\mathbf{w}_0$, $\boldsymbol{\lambda}_0$, $\mathbf{U}_0$ (initial parameter estimates),
 $\alpha_1$, $\alpha_2$ (lower and upper bound on the eigenvalues of the Laplacian matrix),
 $\rho$ (how much to increase beta per iteration)}
 \KwResult{$\boldsymbol{\Theta}$ (Laplacian matrix)}
 $N \gets \texttt{ncol}(\mathbf{Y})$\;
 \While{objective function do not converged or max \#\{iterations\} not reached}{
   $k \gets 0$\;
   \While{parameters do not converged or max \#\{iterations\} not reached}{
  $\mathbf{w}^{(k+1)} \gets \texttt{w\_update}(\mathbf{w}^{(k)}, \mathbf{U}^{(k)}, \boldsymbol{\lambda}^{(k)}, \beta, N, \mathbf{K})$\;
  $\mathbf{U}^{(k+1)} \gets \texttt{U\_update}(\mathbf{w}^{(k+1)}, N)$\;
  $\boldsymbol{\lambda}^{(k+1)} \gets \texttt{lambda\_update}(\mathbf{w}^{(k+1)},
  \mathbf{U}^{(k+1)}, \alpha_1, \alpha_2, \beta,N, K)$\;
  $k \gets k + 1$\;
  }
  $\beta \gets \beta (\rho + 1)$\;
 }
 \Return $\mathcal{L}(\mathbf{w}^{(k+1)})$\;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwupdate}{\texttt{w\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwupdate{$\mathbf{w}, \mathbf{U}, \boldsymbol{\lambda}, \beta, N, \mathbf{K}$}}{
        $\nabla_{\mathbf{w}}f \gets \mathcal{L}^{\star}\left(\mathcal{L}\left(\mathbf{w}\right)
                        - \mathbf{U}  \texttt{diag}(\boldsymbol{\lambda}) {\mathbf{U}^{T}}
                        + \dfrac{\mathbf{K}}{\beta}\right)$\;
        \KwRet $\texttt{max}\left(0, \mathbf{w} - \dfrac{\nabla_{\mathbf{w}}f}{2N}\right)$\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FwUupdate}{\texttt{U\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FwUupdate{$\mathbf{w}, N$}}{
        \KwRet \texttt{eigen}$(\mathcal{L}(\mathbf{w}))$\$\texttt{vectors}$[, N:1]$\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwlambdaupdate}{\texttt{lambda\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwlambdaupdate{$\mathbf{w}, \mathbf{U}, \alpha_1, \alpha_2, \beta, N, K$}}{
        $\mathbf{d} \gets \texttt{diag}\left(\mathbf{U}^{T} \mathcal{L}(\mathbf{w})\mathbf{U}\right)$\;
        $\boldsymbol{\lambda} \gets \frac{1}{2}\left(\mathbf{d} +
                \sqrt{\mathbf{d} \odot \mathbf{d} + \frac{4}{\beta}}\right)$\;
        \eIf{$\boldsymbol{\lambda}$ has its elements in increasing order} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            set to $\alpha_1$ the elements of $\boldsymbol{\lambda}$
            whose values are less than $\alpha_1$ \;
            set to $\alpha_2$ the elements of $\boldsymbol{\lambda}$
            whose values are greater than $\alpha_2$\;
        }
        \eIf{$\boldsymbol{\lambda}$ has its elements in increasing order} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            $\mathbf{raise}$ \texttt{Exception}(\textit{"eigenvalues are not in increasing order"})\;
        }
  }
  \;
\end{algorithm}

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
