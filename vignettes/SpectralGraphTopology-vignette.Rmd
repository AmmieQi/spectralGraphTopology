---
title: "Learning the topology of graphs"
author: "Convex Group-HKUST"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
header-includes:
  \usepackage[]{algorithm2e}
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Learning the topology of graphs}
  %\VignetteKeyword{graph, topology, clustering}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "all")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/SpectralGraphTopology-vignette.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/SpectralGraphTopology-vignette.pdf", gs_quality = "ebook")
```

-----------

# Installation

Please visit [https://github.com/dppalomar/spectralGraphTopology](https://github.com/dppalomar/spectralGraphTopology)
for installation instructions.

# Problem Statement
The Laplacian matrix $\boldsymbol{\Theta}$ of a graph contains the information
of its topology. By definition a Laplacian matrix is positive semi-definite,
symmetric, and with sum of rows equal to zero. The Laplacian linear operator
$\mathcal{L}\mathbf{w}$ maps a vector of weights $\mathbf{w}$ into a valid
Laplacian matrix so that the conditions are satisfied by construction.

One common approach to estimate the Laplacian matrix would be via the generalized
inverse of the covariance matrix, which is an assymptotically unbiased and efficient
estimator. In this document, we call this approach the \textit{naive} one.
In R, this estimator can be computed as ``MASS::ginv(cov(Y))``,
where``Y`` is the data matrix.

Another classical approach was proposed in [@Friedman2008] which incorporates a
$\ell_1$-norm penalty term in order to induce sparsity on the solution. The R package
``glasso`` provides an implementation of this estimator.

The underlying optimization problem of learning a K-component graph, solved by
``spectralGraphTopology``, may be expressed as follows:
$$\begin{array}{ll}
\underset{\mathbf{w}, \boldsymbol{\Lambda}, \mathbf{U}}{\textsf{minimize}} & - \log \det(\boldsymbol{\Lambda})
+ \mathrm{tr}\left(\mathbf{K}\mathcal{L}\mathbf{w}\right)
+ \frac{\beta}{2}\big\|\mathcal{L}\mathbf{w} - \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{T}\big\|^{2}_{F}\\
\textsf{subject to} & \mathbf{w} \geq 0, \boldsymbol{\Lambda} \in \mathcal{S}_{\boldsymbol{\Lambda}},~\text{and}~
\mathbf{U}^{T}\mathbf{U} = \mathbf{I}
\end{array}$$
where $\mathcal{S}_{\boldsymbol{\Lambda}}$ further constrains the eigenvalues of Laplacian matrices
$\boldsymbol{\Theta}=\mathcal{L}\mathbf{w}$ according to some topology (e.g., a $K$-component graph has $K$ zero eigenvalues).

In order to solve this problem, we use a block coordinate descent algorithm to
iteratively optimize each variable while helding the others fixed. For
details check the paper or the algorithm description at the end of this document.

# Usage of the package
We illustrate the usage of the package with simulated data, as follows:
```{r}
library(spectralGraphTopology)
set.seed(123)

# Number of samples
n <- 10
# Vector to generate the Laplacian matrix of the graph
w <- runif(10)
# Laplacian matrix
Lw <- L(w)
# Sample data from a Multivariate Gaussian
p <- ncol(Lw)
Y <- MASS::mvrnorm(n, rep(0, p), MASS::ginv(Lw))
# Number of components of the graph
k <- 1
# Learn the Laplacian matrix
res <- learn_laplacian_matrix(cov(Y), k, beta = 10)
mean((Lw - res$Lw) ^ 2)
max((Lw - res$Lw) ^ 2 / Lw)
```

Let's visually inspect the true Laplacian and the estimated one:
```{r}
Lw
res$Lw
```

Another visual tool is the correlation matrix:

```{r}
library(corrplot)
par(mfrow = c(1, 2))
corrplot(cov2cor(Lw), mar = c(0, 0, 1, 0))
title("True Laplacian matrix", line = 2.5)
corrplot(cov2cor(res$Lw), mar = c(0, 0, 1, 0))
title("Estimated Laplacian matrix", line = 2.5)
```

Finally, we can also plot the graph structure:
```{r}
# True adjancency matrix
W <- Lw - diag(diag(Lw))
library(qgraph)
par(mfrow = c(1, 2))
qgraph(W, esize = 5, edge.color = "black")
title("True diamond graph", line = 2.5)
qgraph(res$W, esize = 5, edge.color = "black")
title("Estimated diamond graph", line = 2.5)
```

We evaluate the performance of the learning process in a more objective
manner by computing two criterions:

1. relative error
2. percentage improvement in average loss (PRIAL)


```{r}
Lw_naive <- MASS::ginv(cov(Y))
rel_err <- c(proposed = spectralGraphTopology::relativeError(Lw, res$Lw),
             naive = spectralGraphTopology::relativeError(Lw, Lw_naive))
rel_err
```

In this case, the naive estimation of the Laplacian matrix (i.e., generalized
inverse of the sample covariance matrix) performs already quite well since
the ratio $T/N$ is large enough `r T/N` for the sample covariance matrix to be
accurately estimated.

Let's also look at the convergence of the objective function versus elapsed time:
```{r}
plot(res$elapsed_time, res$obj_fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "CPU time [seconds]", ylab = "Objective function")
```

## Illustrative example for K = 2

For K = 2 and $\beta = 100$, we can generate the Laplacian as a block diagonal matrix, as follows
```{r}
library(spectralGraphTopology)
T <- 500
w1 <- runif(3)
w2 <- runif(6)
Lw1 <- L(w1)
Lw2 <- L(w2)
N1 <- ncol(Lw1)
N2 <- ncol(Lw2)
Lw <- blockDiag(Lw1, Lw2)
Y <- MASS::mvrnorm(T, rep(0, N1 + N2), MASS::ginv(Lw))
K <- 2
res <- learnGraphTopology(cov(Y), K, beta = 100)

plot(res$elapsed_time, res$obj_fun, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "CPU time [seconds]", ylab = "Objective function")
plot(res$elapsed_time, res$loglike, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "CPU time [seconds]", ylab = "Negative Log Likelihood")

rel_err_seq <- c()
w_seq <- res$w_seq
N_iter <- length(res$obj_fun)
for (i in 1:N_iter) {
  rel_err <- relativeError(Lw, L(w_seq[[i]]))
  rel_err_seq <- c(rel_err_seq, rel_err)
}
plot(res$elapsed_time, rel_err_seq, type = "b", pch=19, cex=.6, col = scales::alpha("black", .5),
     xlab = "CPU time [seconds]", ylab = "Relative Error")

W <- Lw - diag(diag(Lw))
Lw_naive <- MASS::ginv(cov(Y))
W_naive <- -(Lw_naive - diag(diag(Lw_naive)))

library(qgraph)
par(mfrow = c(1, 3))
qgraph(W, esize = 5, edge.color = "black")
title("True graph (K = 2)", line = 2.5)
qgraph(res$W, esize = 5, edge.color = "black")
title("Estimated graph", line = 2.5)
qgraph(W_naive, esize = 5, edge.color = "black")
title("Naive Estimated graph", line = 2.5)
```
# Explanation of the algorithms

In this section we present the algorithms designed to solve the
graph topology learning problem.

## `learnGraphTopology`: Learning the topology of graph

The goal of `learnGraphTopology` is to estimate the Laplacian matrix generated
by the weight vector of a graph, $\mathbf{w}$. The algorithm for the function `learnGraphTopology`
is stated as follows:

\begin{algorithm}[H]
 \DontPrintSemicolon
 \KwData{$\mathbf{Y}$ (data matrix), $K$ (\#\{components\}), $\beta$ (regularization term),
 $\mathbf{w}^{(0)}$, $\boldsymbol{\lambda}^{(0)}$, $\mathbf{U}^{(0)}$ (initial parameter estimates),
 $\alpha_1$, $\alpha_2$ (lower and upper bound on the eigenvalues of the Laplacian matrix),
 $\rho$ (how much to increase beta per iteration).}
 \KwResult{$\boldsymbol{\Theta}$ (Laplacian matrix)}
 $N \gets \texttt{ncol}(\mathbf{Y})$\;
 \While{objective function do not converged \textbf{or} max \#\{iterations\} not reached}{
   $k \gets 0$\;
   \While{parameters do not converged \textbf{or} max \#\{iterations\} not reached}{
  $\mathbf{w}^{(k+1)} \gets \texttt{w\_update}(\mathbf{w}^{(k)}, \mathbf{U}^{(k)}, \boldsymbol{\lambda}^{(k)}, \beta, N, \mathbf{K})$\;
  $\mathbf{U}^{(k+1)} \gets \texttt{U\_update}(\mathbf{w}^{(k+1)}, N)$\;
  $\boldsymbol{\lambda}^{(k+1)} \gets \texttt{lambda\_update}(\mathbf{w}^{(k+1)},
  \mathbf{U}^{(k+1)}, \alpha_1, \alpha_2, \beta,N, K)$\;
  $k \gets k + 1$\;
  }
  $\beta \gets \beta (\rho + 1)$\;
 }
 \Return $\mathcal{L}(\mathbf{w}^{(k+1)})$\;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwupdate}{\texttt{w\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwupdate{$\mathbf{w}, \mathbf{U}, \boldsymbol{\lambda}, \beta, N, \mathbf{K}$}}{
        $\nabla_{\mathbf{w}}f \gets \mathcal{L}^{\star}\left(\mathcal{L}\left(\mathbf{w}\right)
                        - \mathbf{U}  \texttt{diag}(\boldsymbol{\lambda}) {\mathbf{U}^{T}}
                        + \dfrac{\mathbf{K}}{\beta}\right)$\;
        \KwRet $\texttt{max}\left(0, \mathbf{w} - \dfrac{\nabla_{\mathbf{w}}f}{2N}\right)$\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FwUupdate}{\texttt{U\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FwUupdate{$\mathbf{w}, N, K$}}{
        \KwRet \texttt{eigenvectors}$(\mathcal{L}(\mathbf{w}))$[K+1:N] \# \textit{increasing order w.r.t. eigenvalues}\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{Fwlambdaupdate}{\texttt{lambda\_update}}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\Fwlambdaupdate{$\mathbf{w}, \mathbf{U}, \alpha_1, \alpha_2, \beta, N, K$}}{
        $\mathbf{d} \gets \texttt{diag}\left(\mathbf{U}^{T} \mathcal{L}(\mathbf{w})\mathbf{U}\right)$\;
        $\boldsymbol{\lambda} \gets \frac{1}{2}\left(\mathbf{d} +
                \sqrt{\mathbf{d} \odot \mathbf{d} + \frac{4}{\beta}}\right)$
                \# $\odot$ \textit{means element-wise multiplication} \;
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order
             \textbf{and} $\min(\boldsymbol{\lambda}) \geq \alpha_1$
             \textbf{and} $\max(\boldsymbol{\lambda}) \leq \alpha_2$} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            set to $\alpha_1$ the elements of $\boldsymbol{\lambda}$
            whose values are less than $\alpha_1$ \;
            set to $\alpha_2$ the elements of $\boldsymbol{\lambda}$
            whose values are greater than $\alpha_2$\;
        }
        \eIf{$\boldsymbol{\lambda}$ has its elements in nondecreasing order} {
            \KwRet $\boldsymbol{\lambda}$\;
        } {
            $\mathbf{raise}$ \texttt{Exception}(\textit{"eigenvalues are not in increasing order"})\;
        }
  }
  \;
\end{algorithm}

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
