% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learnGraphTopology.R
\name{learn_laplacian_matrix}
\alias{learn_laplacian_matrix}
\title{Learn Graph Topology}
\usage{
learn_laplacian_matrix(S, is_data_matrix = FALSE, k = 1,
  w0 = "naive", lb = 0, ub = 10000, alpha = 0, beta = 10000,
  beta_max = 1e+06, fix_beta = FALSE, rho = 0.01, m = 7,
  maxiter = 10000, abstol = 1e-06, reltol = 1e-04, eig_tol = 1e-09,
  record_objective = FALSE, record_weights = FALSE)
}
\arguments{
\item{S}{N-by-N sample covariance matrix, where N is the number of nodes}

\item{w0}{initial estimate for the weight vector the graph or a string
selecting an appropriate method. Available methods are: "qp": solves a
simple quadratic program; "naive": sets w0 to the negative of the
off-diagonal elements of the generalized precision matrix; "glasso": uses
the glasso solution}

\item{lb}{lower bound for the eigenvalues of the Laplacian matrix}

\item{ub}{upper bound for the eigenvalues of the Laplacian matrix}

\item{alpha}{tunning parameter}

\item{beta}{parameter that controls the strength of the regularization term}

\item{rho}{how much to increase beta after a complete round of iterations}

\item{maxiter}{the maximum number of iterations for each beta}

\item{K}{the number of components of the graph}

\item{maxiter_beta}{the maximum number of iterations for the outer loop}

\item{Lwtol}{relative tolerance on the Laplacian matrix}

\item{ftol}{relative tolerance on the objective function}
}
\value{
Lw the learned Laplacian matrix

W the learned weighted adjacency matrix

obj_fun the objective function value at every iteration

loglike the negative loglikelihood function value at every iteration

w the optimal value of the weight vector

lambda the optimal value of the eigenvalues

U the optimal value of the eigenvectors
}
\description{
Learns the topology of a K-connected graph given an observed data matrix
}
\examples{
library(spectralGraphTopology)

# simulate a Laplacian matrix of a single-component graph
w <- sample(1:10, 10)
Lw <- L(w)

# create fake data
T <- 40
N <- ncol(Lw)
Y <- MASS::mvrnorm(T, rep(0, N), MASS::ginv(Lw))

# learn the Laplacian matrix from the simulated data
res <- learn_laplacian_matrix(cov(Y))

# relative error between the true Laplacian and the learned one
norm(Lw - res$Lw, type="F") / norm(Lw, type="F")
}
\references{
our paper soon to be submitted
}
\author{
Ze Vinicius and Daniel Palomar
}
